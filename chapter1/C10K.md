# C10K 문제

## 뭔데?

1999년 Dan Kegel이 제기한 문제. "어떻게 하면 단일 서버에서 동시에 10,000개의 클라이언트 연결을 처리할 수 있을까?"

그 당시 웹서버들은 연결 하나당 프로세스/스레드 하나를 할당하는 방식이었음. 10,000개 연결 = 10,000개 스레드. 컨텍스트 스위칭 비용만으로 서버가 뻗어버림.

## select()의 한계

```
kernel.py의 check_ready()를 보면:

def check_ready(self, fd: int, event_type: str) -> bool:
    if fd not in self.sockets:
        return False
    if event_type == "read":
        return self.sockets[fd].has_data_to_read()
    ...
```

`selector.py`의 select() 구현을 보면 문제가 보임:

```python
while True:
    for fd in read_fds:          # <-- 매번 전체 fd를 순회
        if self.kernel.check_ready(fd, "read"):
            ready_read.append(fd)
```

10,000개 연결이 있으면?
- 유저 → 커널로 10,000개 fd 복사 (매 호출마다)
- 커널에서 10,000개 전부 체크
- 결과를 다시 유저로 복사

실제로 데이터가 온 건 딱 3개뿐인데, 10,000개를 다 뒤져봐야 함. O(n) 복잡도.

## epoll이 해결한 방식

`epoll.py` 구조를 보면:

```python
class Epoll:
    def __init__(self, kernel: KernelSimulator) -> None:
        self.interest_list: Dict[int, int] = {}  # 한번 등록하면 커널이 보관
        self.ready_list: List[EpollEvent] = []   # 준비된 것만 여기에
```

핵심 차이:

1. **epoll_ctl_add()** - fd를 커널의 interest_list에 등록 (최초 1회)
2. **epoll_wait()** - ready_list에서 준비된 것만 가져옴

```
select():  [유저] ---(10,000개 fd)---> [커널] ---(순회)---> [유저]
                         매번 복사              O(n)

epoll():   [유저] <---(ready 3개)--- [커널이 알아서 관리]
                     준비된 것만             O(1) 등록, O(ready) 반환
```

커널이 네트워크 인터럽트 받을 때 직접 ready_list에 추가해줌. 유저는 기다리다가 받기만 하면 됨.

## 지금 코드와의 연결

```
chapter1/
├── kernel.py      # 커널 시뮬레이터 (소켓, fd, 버퍼)
├── selector.py    # select() 시스템 콜 - O(n) 방식
├── epoll.py       # epoll() 시스템 콜 - O(1) 방식
└── benchmark.py   # 2000개 fd로 비교
```

benchmark.py 결과에서 봤듯이:
- select는 매번 2000개 fd를 커널에 전달
- epoll은 등록 후 wait만 호출

2000개에서도 차이가 나는데, 10,000개면? 100,000개면?

## 다음 단계

지금까지 "커널이 I/O를 어떻게 관리하는가"를 봤음. 이게 asyncio의 기반.

다음 챕터에서 할 것:
- 이 epoll 위에 이벤트 루프 올리기
- 콜백 대신 코루틴으로 감싸기
- async/await 문법으로 예쁘게 만들기

결국 asyncio도 내부적으로 epoll (리눅스) / kqueue (맥) / IOCP (윈도우) 씀.
우리가 `await` 쓸 때마다 뒤에서는 이 시스템 콜들이 돌아가고 있는 것.
